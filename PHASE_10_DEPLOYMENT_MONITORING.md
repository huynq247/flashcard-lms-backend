# üöÄ PHASE 10: DEPLOYMENT & MONITORING
*Production deployment, monitoring, and operational excellence*

## üìã Overview
**Phase Goal**: Deploy to production with comprehensive monitoring  
**Dependencies**: Phase 9 (Performance Optimization)  
**Estimated Time**: 4-5 days  
**Priority**: HIGH PRIORITY

---

## üéØ PHASE OBJECTIVES

### **10.1 Production Deployment**
- [ ] Docker containerization
- [ ] Production environment setup
- [ ] Database deployment and migration

### **10.2 CI/CD Pipeline**
- [ ] Automated testing and deployment
- [ ] Environment management
- [ ] Release management

### **10.3 Monitoring & Logging**
- [ ] Application monitoring
- [ ] Error tracking and alerting
- [ ] Performance monitoring

### **10.4 Security & Backup**
- [ ] Production security hardening
- [ ] Backup and disaster recovery
- [ ] SSL/TLS configuration

---

## üê≥ CONTAINERIZATION

### **10.1 Docker Configuration**

#### **Backend Dockerfile**
```dockerfile
# Dockerfile.backend
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt requirements-prod.txt ./

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir -r requirements-prod.txt

# Copy application code
COPY . .

# Create non-root user
RUN adduser --disabled-password --gecos '' appuser \
    && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### **Frontend Dockerfile**
```dockerfile
# Dockerfile.frontend
# Multi-stage build
FROM node:18-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy source code
COPY . .

# Build application
RUN npm run build

# Production stage
FROM nginx:alpine

# Copy custom nginx config
COPY nginx.conf /etc/nginx/nginx.conf

# Copy built application
COPY --from=builder /app/dist /usr/share/nginx/html

# Add labels for monitoring
LABEL app="flashcard-lms-frontend"
LABEL version="1.0.0"

# Expose port
EXPOSE 80

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:80/ || exit 1

CMD ["nginx", "-g", "daemon off;"]
```

#### **Docker Compose for Production**
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.backend
    container_name: flashcard-backend
    restart: unless-stopped
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - ENVIRONMENT=production
    depends_on:
      - mongodb
      - redis
    networks:
      - app-network
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.backend.rule=Host(`api.flashcard-lms.com`)"
      - "traefik.http.routers.backend.tls=true"
      - "traefik.http.routers.backend.tls.certresolver=letsencrypt"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.frontend
    container_name: flashcard-frontend
    restart: unless-stopped
    networks:
      - app-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=Host(`flashcard-lms.com`)"
      - "traefik.http.routers.frontend.tls=true"
      - "traefik.http.routers.frontend.tls.certresolver=letsencrypt"

  mongodb:
    image: mongo:6.0
    container_name: flashcard-mongodb
    restart: unless-stopped
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DATABASE}
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init:/docker-entrypoint-initdb.d
    networks:
      - app-network
    command: mongod --auth --bind_ip_all

  redis:
    image: redis:7-alpine
    container_name: flashcard-redis
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - app-network

  traefik:
    image: traefik:v2.9
    container_name: flashcard-traefik
    restart: unless-stopped
    command:
      - "--api.dashboard=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/acme.json"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web"
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"  # Traefik dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./acme.json:/acme.json
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  mongodb_data:
  redis_data:
```

#### **Nginx Configuration**
```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # Performance settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private must-revalidate auth;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json;

    server {
        listen 80;
        server_name _;
        root /usr/share/nginx/html;
        index index.html;

        # Security headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;

        # Cache static assets
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # SPA routing
        location / {
            try_files $uri $uri/ /index.html;
        }

        # Health check
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
```

#### **Implementation Checklist**
- [ ] **Containerization**
  - [ ] Backend Docker configuration
  - [ ] Frontend Docker configuration
  - [ ] Multi-stage builds for optimization
  - [ ] Health checks for containers

### **10.2 Environment Configuration**

#### **Environment Variables Management**
```bash
# .env.production
# Database
DATABASE_URL=mongodb://username:password@mongodb:27017/flashcard_lms
MONGO_ROOT_USERNAME=admin
MONGO_ROOT_PASSWORD=secure_password_here
MONGO_DATABASE=flashcard_lms

# Redis
REDIS_URL=redis://:password@redis:6379/0
REDIS_PASSWORD=secure_redis_password

# JWT
JWT_SECRET_KEY=your-super-secure-jwt-secret-key-here
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=30

# Application
ENVIRONMENT=production
DEBUG=false
API_BASE_URL=https://api.flashcard-lms.com
FRONTEND_URL=https://flashcard-lms.com

# File Storage
UPLOAD_PATH=/app/uploads
MAX_FILE_SIZE=10485760  # 10MB

# Email (if using)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# SSL
ACME_EMAIL=admin@flashcard-lms.com

# Monitoring
SENTRY_DSN=your-sentry-dsn-here
```

#### **Production Configuration**
```python
# app/core/config.py (Enhanced for production)
from pydantic import BaseSettings, Field
from typing import Optional

class Settings(BaseSettings):
    # Application
    app_name: str = "Flashcard LMS"
    environment: str = Field(default="development")
    debug: bool = Field(default=False)
    
    # Database
    database_url: str = Field(..., env="DATABASE_URL")
    database_name: str = Field(default="flashcard_lms")
    
    # Redis
    redis_url: str = Field(..., env="REDIS_URL")
    
    # JWT
    jwt_secret_key: str = Field(..., env="JWT_SECRET_KEY")
    jwt_algorithm: str = Field(default="HS256")
    jwt_expire_minutes: int = Field(default=30)
    
    # CORS
    cors_origins: list = Field(default=["https://flashcard-lms.com"])
    
    # File Upload
    upload_path: str = Field(default="/app/uploads")
    max_file_size: int = Field(default=10485760)  # 10MB
    
    # Rate Limiting
    rate_limit_requests_per_minute: int = Field(default=60)
    rate_limit_requests_per_hour: int = Field(default=1000)
    
    # Security
    password_min_length: int = Field(default=8)
    session_timeout_minutes: int = Field(default=60)
    
    # Monitoring
    sentry_dsn: Optional[str] = Field(default=None, env="SENTRY_DSN")
    log_level: str = Field(default="INFO")
    
    # Email
    smtp_host: Optional[str] = Field(default=None, env="SMTP_HOST")
    smtp_port: int = Field(default=587)
    smtp_username: Optional[str] = Field(default=None, env="SMTP_USERNAME")
    smtp_password: Optional[str] = Field(default=None, env="SMTP_PASSWORD")
    
    @property
    def is_production(self) -> bool:
        return self.environment.lower() == "production"
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```

#### **Implementation Checklist**
- [ ] **Environment Management**
  - [ ] Environment variable configuration
  - [ ] Production settings validation
  - [ ] Security configuration
  - [ ] CORS configuration

---

## üîÑ CI/CD PIPELINE

### **10.3 GitHub Actions Workflow**

#### **Main CI/CD Pipeline**
```yaml
# .github/workflows/deploy.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: flashcard-lms

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      mongodb:
        image: mongo:6.0
        env:
          MONGO_INITDB_ROOT_USERNAME: test
          MONGO_INITDB_ROOT_PASSWORD: test
        ports:
          - 27017:27017
          
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install Python dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Install Node dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run backend tests
      env:
        DATABASE_URL: mongodb://test:test@localhost:27017/test_flashcard_lms
        REDIS_URL: redis://localhost:6379/0
        JWT_SECRET_KEY: test-secret-key
      run: |
        cd backend
        pytest --cov=app --cov-report=xml
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test:coverage
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: ./backend/coverage.xml,./frontend/coverage/lcov.info

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: security-scan-results.sarif

  build:
    needs: [test, security]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.DOCKER_REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-

    - name: Build and push backend image
      uses: docker/build-push-action@v4
      with:
        context: ./backend
        file: ./backend/Dockerfile.backend
        push: true
        tags: ${{ steps.meta.outputs.tags }}-backend
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build and push frontend image
      uses: docker/build-push-action@v4
      with:
        context: ./frontend
        file: ./frontend/Dockerfile.frontend
        push: true
        tags: ${{ steps.meta.outputs.tags }}-frontend
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PRODUCTION_HOST }}
        username: ${{ secrets.PRODUCTION_USER }}
        key: ${{ secrets.PRODUCTION_SSH_KEY }}
        script: |
          cd /opt/flashcard-lms
          docker-compose -f docker-compose.prod.yml pull
          docker-compose -f docker-compose.prod.yml up -d
          docker system prune -f
```

#### **Database Migration Workflow**
```yaml
# .github/workflows/migrate.yml
name: Database Migration

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to migrate'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

jobs:
  migrate:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
    
    - name: Run migrations
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |
        cd backend
        python -m app.migrations.run_migrations
```

#### **Implementation Checklist**
- [ ] **CI/CD Pipeline**
  - [ ] Automated testing pipeline
  - [ ] Security scanning
  - [ ] Docker image building
  - [ ] Automated deployment

### **10.4 Deployment Scripts**

#### **Production Deployment Script**
```bash
#!/bin/bash
# scripts/deploy_production.sh

set -e

echo "üöÄ Starting production deployment..."

# Configuration
PROJECT_DIR="/opt/flashcard-lms"
BACKUP_DIR="/opt/backups/flashcard-lms"
DATE=$(date +%Y%m%d_%H%M%S)

# Create backup directory
mkdir -p $BACKUP_DIR

# Function to rollback on failure
rollback() {
    echo "‚ùå Deployment failed! Rolling back..."
    cd $PROJECT_DIR
    docker-compose -f docker-compose.prod.yml down
    docker-compose -f docker-compose.prod.yml.backup up -d
    exit 1
}

# Set trap for rollback
trap rollback ERR

# Backup current deployment
echo "üì¶ Creating backup..."
cp docker-compose.prod.yml docker-compose.prod.yml.backup
docker-compose -f docker-compose.prod.yml exec mongodb mongodump --out /backup/mongodb_$DATE

# Pull latest changes
echo "üì• Pulling latest changes..."
git pull origin main

# Update environment variables
echo "üîß Updating configuration..."
source .env.production

# Pull latest images
echo "üê≥ Pulling latest Docker images..."
docker-compose -f docker-compose.prod.yml pull

# Stop services gracefully
echo "‚èπÔ∏è Stopping services..."
docker-compose -f docker-compose.prod.yml down --timeout 30

# Start services
echo "‚ñ∂Ô∏è Starting services..."
docker-compose -f docker-compose.prod.yml up -d

# Health check
echo "üè• Performing health checks..."
sleep 30

# Check backend health
if ! curl -f http://localhost:8000/health; then
    echo "‚ùå Backend health check failed"
    rollback
fi

# Check frontend health  
if ! curl -f http://localhost:80/health; then
    echo "‚ùå Frontend health check failed"
    rollback
fi

# Check database connectivity
if ! docker-compose -f docker-compose.prod.yml exec backend python -c "from app.core.database import ping_database; ping_database()"; then
    echo "‚ùå Database connectivity check failed"
    rollback
fi

# Cleanup old images
echo "üßπ Cleaning up..."
docker system prune -f

echo "‚úÖ Production deployment completed successfully!"
echo "üìä Application is running at: https://flashcard-lms.com"
```

#### **Database Migration Script**
```bash
#!/bin/bash
# scripts/migrate_database.sh

set -e

echo "üóÉÔ∏è Starting database migration..."

# Configuration
ENVIRONMENT=${1:-staging}
BACKUP_PREFIX="migration_backup_$(date +%Y%m%d_%H%M%S)"

echo "Environment: $ENVIRONMENT"

# Create backup before migration
echo "üì¶ Creating database backup..."
docker-compose -f docker-compose.prod.yml exec mongodb mongodump --db flashcard_lms --out /backup/$BACKUP_PREFIX

# Run migrations
echo "üîÑ Running database migrations..."
docker-compose -f docker-compose.prod.yml exec backend python -m app.migrations.run_migrations

# Verify migration
echo "‚úÖ Verifying migration..."
docker-compose -f docker-compose.prod.yml exec backend python -m app.migrations.verify_migrations

echo "‚úÖ Database migration completed successfully!"
```

#### **Implementation Checklist**
- [ ] **Deployment Scripts**
  - [ ] Production deployment automation
  - [ ] Database migration scripts
  - [ ] Rollback procedures
  - [ ] Health check validation

---

## üìä MONITORING & LOGGING

### **10.5 Application Monitoring**

#### **Prometheus Configuration**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'flashcard-backend'
    static_configs:
      - targets: ['backend:8000']
    scrape_interval: 5s
    metrics_path: /metrics
    
  - job_name: 'flashcard-frontend'
    static_configs:
      - targets: ['frontend:80']
    scrape_interval: 30s
    
  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb-exporter:9216']
      
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

#### **FastAPI Metrics Integration**
```python
# app/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Request, Response
from fastapi.middleware.base import BaseHTTPMiddleware
import time

# Metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_USERS = Gauge(
    'active_users_total',
    'Number of active users'
)

STUDY_SESSIONS = Counter(
    'study_sessions_total',
    'Total study sessions started',
    ['user_id', 'deck_id']
)

DATABASE_OPERATIONS = Histogram(
    'database_operation_duration_seconds',
    'Database operation duration',
    ['operation', 'collection']
)

class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        
        response = await call_next(request)
        
        # Record metrics
        duration = time.time() - start_time
        
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=str(request.url.path),
            status_code=response.status_code
        ).inc()
        
        REQUEST_DURATION.labels(
            method=request.method,
            endpoint=str(request.url.path)
        ).observe(duration)
        
        return response

# Metrics endpoint
async def metrics_endpoint():
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )

# Custom metrics functions
def track_study_session(user_id: str, deck_id: str):
    STUDY_SESSIONS.labels(user_id=user_id, deck_id=deck_id).inc()

def track_database_operation(operation: str, collection: str, duration: float):
    DATABASE_OPERATIONS.labels(operation=operation, collection=collection).observe(duration)

def update_active_users(count: int):
    ACTIVE_USERS.set(count)
```

#### **Grafana Dashboard Configuration**
```json
{
  "dashboard": {
    "title": "Flashcard LMS Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph", 
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "Median"
          }
        ]
      },
      {
        "title": "Active Users",
        "type": "stat",
        "targets": [
          {
            "expr": "active_users_total",
            "legendFormat": "Active Users"
          }
        ]
      },
      {
        "title": "Study Sessions",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(study_sessions_total[1h])",
            "legendFormat": "Sessions per hour"
          }
        ]
      }
    ]
  }
}
```

#### **Implementation Checklist**
- [ ] **Application Monitoring**
  - [ ] Prometheus metrics collection
  - [ ] Grafana dashboard setup
  - [ ] Custom business metrics
  - [ ] Performance monitoring

### **10.6 Error Tracking and Alerting**

#### **Sentry Integration**
```python
# app/monitoring/sentry_config.py
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.pymongo import PyMongoIntegration
from sentry_sdk.integrations.redis import RedisIntegration
from app.core.config import settings

def initialize_sentry():
    if settings.sentry_dsn:
        sentry_sdk.init(
            dsn=settings.sentry_dsn,
            integrations=[
                FastApiIntegration(auto_enabling_integrations=False),
                PyMongoIntegration(),
                RedisIntegration(),
            ],
            traces_sample_rate=0.1,  # Adjust based on traffic
            profiles_sample_rate=0.1,
            environment=settings.environment,
            release=settings.app_version,
            before_send=filter_sensitive_data,
        )

def filter_sensitive_data(event, hint):
    """Filter sensitive data from Sentry events."""
    
    # Remove password fields
    if 'request' in event and 'data' in event['request']:
        data = event['request']['data']
        if isinstance(data, dict):
            for key in ['password', 'token', 'secret']:
                if key in data:
                    data[key] = '[Filtered]'
    
    return event
```

#### **Alert Rules Configuration**
```yaml
# alert_rules.yml
groups:
  - name: flashcard_lms_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status_code=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} requests per second"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time"
          description: "95th percentile response time is {{ $value }} seconds"

      - alert: DatabaseConnectionDown
        expr: up{job="mongodb"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection is down"
          description: "MongoDB is not responding"

      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space"
          description: "Disk space is below 10%"

      - alert: MemoryUsageHigh
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 80%"
```

#### **Structured Logging**
```python
# app/core/logging.py
import logging
import json
from datetime import datetime
from typing import Dict, Any
from app.core.config import settings

class JSONFormatter(logging.Formatter):
    """JSON formatter for structured logging."""
    
    def format(self, record: logging.LogRecord) -> str:
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno,
        }
        
        # Add exception info if present
        if record.exc_info:
            log_entry['exception'] = self.formatException(record.exc_info)
        
        # Add extra fields
        for key, value in record.__dict__.items():
            if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 'filename',
                          'module', 'lineno', 'funcName', 'created', 'msecs', 'relativeCreated',
                          'thread', 'threadName', 'processName', 'process', 'getMessage', 'exc_info',
                          'exc_text', 'stack_info']:
                log_entry[key] = value
        
        return json.dumps(log_entry)

def setup_logging():
    """Setup application logging configuration."""
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, settings.log_level.upper()))
    
    # Remove default handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JSONFormatter())
    root_logger.addHandler(console_handler)
    
    # Create file handler for production
    if settings.is_production:
        file_handler = logging.FileHandler('/app/logs/application.log')
        file_handler.setFormatter(JSONFormatter())
        root_logger.addHandler(file_handler)
    
    # Configure third-party loggers
    logging.getLogger('uvicorn').setLevel(logging.INFO)
    logging.getLogger('motor').setLevel(logging.WARNING)

# Logging utilities
def log_user_action(user_id: str, action: str, resource_type: str, resource_id: str, **kwargs):
    """Log user actions for audit trail."""
    logger = logging.getLogger('user_actions')
    logger.info(
        f"User action: {action}",
        extra={
            'user_id': user_id,
            'action': action,
            'resource_type': resource_type,
            'resource_id': resource_id,
            **kwargs
        }
    )

def log_performance_metric(operation: str, duration: float, **kwargs):
    """Log performance metrics."""
    logger = logging.getLogger('performance')
    logger.info(
        f"Performance metric: {operation}",
        extra={
            'operation': operation,
            'duration': duration,
            **kwargs
        }
    )
```

#### **Implementation Checklist**
- [ ] **Error Tracking**
  - [ ] Sentry integration for error tracking
  - [ ] Alert rules configuration
  - [ ] Structured logging implementation
  - [ ] Log aggregation setup

### **10.7 Health Checks and Status Pages**

#### **Health Check Implementation**
```python
# app/api/v1/health.py
from fastapi import APIRouter, Depends, HTTPException
from datetime import datetime
from app.core.database import get_database
from app.core.cache import cache_service
from app.monitoring.metrics import ACTIVE_USERS

router = APIRouter(prefix="/health", tags=["health"])

@router.get("/")
async def health_check():
    """Basic health check endpoint."""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0"
    }

@router.get("/detailed")
async def detailed_health_check(db=Depends(get_database)):
    """Detailed health check with dependency status."""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "dependencies": {}
    }
    
    # Check database
    try:
        await db.command("ping")
        health_status["dependencies"]["database"] = {
            "status": "healthy",
            "response_time_ms": 0  # You can measure actual response time
        }
    except Exception as e:
        health_status["dependencies"]["database"] = {
            "status": "unhealthy",
            "error": str(e)
        }
        health_status["status"] = "degraded"
    
    # Check Redis
    try:
        if cache_service.redis_client:
            await cache_service.redis_client.ping()
            health_status["dependencies"]["redis"] = {
                "status": "healthy"
            }
        else:
            health_status["dependencies"]["redis"] = {
                "status": "unhealthy",
                "error": "Redis client not initialized"
            }
    except Exception as e:
        health_status["dependencies"]["redis"] = {
            "status": "unhealthy",
            "error": str(e)
        }
        health_status["status"] = "degraded"
    
    # Add application metrics
    health_status["metrics"] = {
        "active_users": ACTIVE_USERS._value._value if hasattr(ACTIVE_USERS, '_value') else 0,
        "uptime_seconds": 0  # Implement uptime tracking
    }
    
    return health_status

@router.get("/readiness")
async def readiness_check(db=Depends(get_database)):
    """Kubernetes readiness probe."""
    try:
        # Check if app can serve traffic
        await db.command("ping")
        return {"status": "ready"}
    except Exception:
        raise HTTPException(status_code=503, detail="Not ready")

@router.get("/liveness")
async def liveness_check():
    """Kubernetes liveness probe."""
    # Check if app should be restarted
    return {"status": "alive"}
```

#### **Status Page**
```python
# app/api/v1/status.py
from fastapi import APIRouter, Depends
from datetime import datetime, timedelta
from app.services.analytics_service import AnalyticsService

router = APIRouter(prefix="/status", tags=["status"])

@router.get("/")
async def system_status():
    """Public status page information."""
    
    # Get system metrics
    analytics_service = AnalyticsService()
    
    # Calculate uptime (implement based on deployment time)
    uptime_seconds = 86400  # Example: 1 day
    
    # Get recent incidents (implement incident tracking)
    incidents = []  # Fetch from incident management system
    
    return {
        "system": {
            "status": "operational",  # operational, degraded, major_outage
            "uptime": {
                "seconds": uptime_seconds,
                "percentage": 99.9
            }
        },
        "services": {
            "api": {"status": "operational"},
            "frontend": {"status": "operational"},
            "database": {"status": "operational"},
            "cache": {"status": "operational"}
        },
        "recent_incidents": incidents,
        "last_updated": datetime.utcnow().isoformat()
    }
```

#### **Implementation Checklist**
- [ ] **Health Monitoring**
  - [ ] Health check endpoints
  - [ ] Dependency health validation
  - [ ] Kubernetes probes
  - [ ] Public status page

---

## üîí SECURITY & BACKUP

### **10.8 Production Security Hardening**

#### **Security Headers Configuration**
```python
# app/middleware/security.py
from fastapi import Request, Response
from fastapi.middleware.base import BaseHTTPMiddleware

class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """Add security headers to all responses."""
    
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # Security headers
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
        response.headers["Permissions-Policy"] = "geolocation=(), microphone=(), camera=()"
        
        # Content Security Policy
        csp = (
            "default-src 'self'; "
            "script-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net; "
            "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; "
            "font-src 'self' https://fonts.gstatic.com; "
            "img-src 'self' data: https:; "
            "connect-src 'self'"
        )
        response.headers["Content-Security-Policy"] = csp
        
        return response
```

#### **Environment Security Validation**
```python
# app/core/security_validator.py
import os
import secrets
from app.core.config import settings

class SecurityValidator:
    """Validate production security configuration."""
    
    @staticmethod
    def validate_jwt_secret():
        """Ensure JWT secret is secure."""
        jwt_secret = settings.jwt_secret_key
        
        if len(jwt_secret) < 32:
            raise ValueError("JWT secret key must be at least 32 characters")
        
        if jwt_secret in ["your-secret-key", "secret", "password"]:
            raise ValueError("JWT secret key must not be a default value")
    
    @staticmethod
    def validate_database_credentials():
        """Validate database security."""
        db_url = settings.database_url
        
        if "localhost" in db_url and settings.is_production:
            raise ValueError("Production should not use localhost database")
        
        if "admin:admin" in db_url or "root:root" in db_url:
            raise ValueError("Database must not use default credentials")
    
    @staticmethod
    def validate_environment():
        """Validate overall environment security."""
        if settings.is_production:
            if settings.debug:
                raise ValueError("Debug mode must be disabled in production")
            
            if not settings.sentry_dsn:
                print("Warning: Sentry DSN not configured for error tracking")
    
    @classmethod
    def validate_all(cls):
        """Run all security validations."""
        cls.validate_jwt_secret()
        cls.validate_database_credentials()
        cls.validate_environment()
```

#### **Implementation Checklist**
- [ ] **Security Hardening**
  - [ ] Security headers middleware
  - [ ] Environment validation
  - [ ] Credential security checks
  - [ ] HTTPS enforcement

### **10.9 Backup and Disaster Recovery**

#### **Backup Strategy Implementation**
```bash
#!/bin/bash
# scripts/backup_system.sh

set -e

# Configuration
BACKUP_DIR="/opt/backups/flashcard-lms"
RETENTION_DAYS=30
DATE=$(date +%Y%m%d_%H%M%S)
S3_BUCKET="flashcard-lms-backups"

# Create backup directory
mkdir -p $BACKUP_DIR/$DATE

echo "üóÑÔ∏è Starting system backup..."

# Database backup
echo "üì¶ Backing up MongoDB..."
docker-compose -f docker-compose.prod.yml exec -T mongodb mongodump \
    --authenticationDatabase admin \
    --username $MONGO_ROOT_USERNAME \
    --password $MONGO_ROOT_PASSWORD \
    --out /backup/mongodb_$DATE

# Copy database backup from container
docker cp flashcard-mongodb:/backup/mongodb_$DATE $BACKUP_DIR/$DATE/

# Redis backup
echo "üì¶ Backing up Redis..."
docker-compose -f docker-compose.prod.yml exec -T redis redis-cli --rdb /data/dump_$DATE.rdb
docker cp flashcard-redis:/data/dump_$DATE.rdb $BACKUP_DIR/$DATE/

# Application files backup
echo "üì¶ Backing up application files..."
tar -czf $BACKUP_DIR/$DATE/uploads.tar.gz /opt/flashcard-lms/uploads/
tar -czf $BACKUP_DIR/$DATE/logs.tar.gz /opt/flashcard-lms/logs/

# Configuration backup
cp /opt/flashcard-lms/.env.production $BACKUP_DIR/$DATE/
cp /opt/flashcard-lms/docker-compose.prod.yml $BACKUP_DIR/$DATE/

# Create manifest
echo "Creating backup manifest..."
cat > $BACKUP_DIR/$DATE/manifest.json << EOF
{
    "backup_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "backup_type": "full",
    "components": [
        "mongodb",
        "redis", 
        "uploads",
        "logs",
        "configuration"
    ],
    "size_mb": $(du -sm $BACKUP_DIR/$DATE | cut -f1)
}
EOF

# Upload to S3 (if configured)
if [ ! -z "$AWS_ACCESS_KEY_ID" ]; then
    echo "‚òÅÔ∏è Uploading to S3..."
    tar -czf /tmp/backup_$DATE.tar.gz -C $BACKUP_DIR $DATE
    aws s3 cp /tmp/backup_$DATE.tar.gz s3://$S3_BUCKET/
    rm /tmp/backup_$DATE.tar.gz
fi

# Cleanup old backups
echo "üßπ Cleaning up old backups..."
find $BACKUP_DIR -type d -mtime +$RETENTION_DAYS -exec rm -rf {} +

echo "‚úÖ Backup completed successfully!"
echo "üìç Backup location: $BACKUP_DIR/$DATE"
```

#### **Disaster Recovery Procedures**
```bash
#!/bin/bash
# scripts/disaster_recovery.sh

set -e

BACKUP_DATE=${1}
BACKUP_DIR="/opt/backups/flashcard-lms"
RESTORE_DIR="/opt/flashcard-lms-restore"

if [ -z "$BACKUP_DATE" ]; then
    echo "Usage: $0 <backup_date>"
    echo "Available backups:"
    ls -la $BACKUP_DIR/
    exit 1
fi

echo "üîÑ Starting disaster recovery from backup: $BACKUP_DATE"

# Verify backup exists
if [ ! -d "$BACKUP_DIR/$BACKUP_DATE" ]; then
    echo "‚ùå Backup not found: $BACKUP_DATE"
    exit 1
fi

# Create restore directory
mkdir -p $RESTORE_DIR

# Stop current services
echo "‚èπÔ∏è Stopping current services..."
cd /opt/flashcard-lms
docker-compose -f docker-compose.prod.yml down

# Restore MongoDB
echo "üóÉÔ∏è Restoring MongoDB..."
docker run --rm -v $BACKUP_DIR/$BACKUP_DATE:/backup \
    -v mongodb_data:/data/db \
    mongo:6.0 \
    bash -c "mongorestore --drop /backup/mongodb_$BACKUP_DATE"

# Restore Redis
echo "üóÉÔ∏è Restoring Redis..."
docker run --rm -v $BACKUP_DIR/$BACKUP_DATE:/backup \
    -v redis_data:/data \
    redis:7-alpine \
    bash -c "cp /backup/dump_$BACKUP_DATE.rdb /data/dump.rdb"

# Restore application files
echo "üìÅ Restoring application files..."
cd $RESTORE_DIR
tar -xzf $BACKUP_DIR/$BACKUP_DATE/uploads.tar.gz
tar -xzf $BACKUP_DIR/$BACKUP_DATE/logs.tar.gz

# Restore configuration
cp $BACKUP_DIR/$BACKUP_DATE/.env.production /opt/flashcard-lms/
cp $BACKUP_DIR/$BACKUP_DATE/docker-compose.prod.yml /opt/flashcard-lms/

# Start services
echo "‚ñ∂Ô∏è Starting restored services..."
cd /opt/flashcard-lms
docker-compose -f docker-compose.prod.yml up -d

# Health check
echo "üè• Performing health check..."
sleep 30

if curl -f http://localhost:8000/health; then
    echo "‚úÖ Disaster recovery completed successfully!"
else
    echo "‚ùå Health check failed after restore"
    exit 1
fi
```

#### **Implementation Checklist**
- [ ] **Backup & Recovery**
  - [ ] Automated backup scripts
  - [ ] Disaster recovery procedures
  - [ ] Backup verification
  - [ ] Recovery testing

---

## üìã COMPLETION CRITERIA

‚úÖ **Phase 10 Complete When:**
- [ ] Docker containers built and tested
- [ ] Production environment deployed
- [ ] CI/CD pipeline operational
- [ ] Monitoring and alerting configured
- [ ] Security hardening implemented
- [ ] Backup and recovery procedures tested
- [ ] SSL/TLS certificates configured
- [ ] Performance monitoring active
- [ ] Error tracking operational
- [ ] Documentation updated

---

## üéâ PROJECT COMPLETION

**üéä Congratulations! Flashcard LMS is now production-ready!**

### **Final System Architecture:**
- ‚úÖ **Backend**: FastAPI + MongoDB + Redis
- ‚úÖ **Frontend**: React TypeScript with modern tooling
- ‚úÖ **Authentication**: JWT with role-based access
- ‚úÖ **Learning System**: SM-2 spaced repetition algorithm
- ‚úÖ **3-Level Hierarchy**: Classes ‚Üí Courses ‚Üí Lessons
- ‚úÖ **Import/Export**: CSV, JSON, and Anki support
- ‚úÖ **Monitoring**: Prometheus + Grafana + Sentry
- ‚úÖ **Deployment**: Docker + CI/CD + Production ready

### **Live System URLs:**
- üåê **Frontend**: https://flashcard-lms.com
- üîó **API**: https://api.flashcard-lms.com
- üìä **Monitoring**: https://monitor.flashcard-lms.com
- üìà **Status**: https://status.flashcard-lms.com

---

*üéØ Comprehensive Flashcard LMS implementation completed successfully!*  
*üìö Based on 20 strategic decisions from DECISION_FRAMEWORK.md*  
*üöÄ Ready for production use and scaling*
